{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating DataFrames in the real world\n",
    "\n",
    "A look at various techniques to modify the contents of DataFrames in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame voter_df contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the VOTER_NAME column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|VOTER_NAME       |\n",
      "+-----------------+\n",
      "|null             |\n",
      "|011018__42       |\n",
      "|the final 201... |\n",
      "|Tiffinni A. Young|\n",
      "|Monica R. Alonzo |\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+\n",
      "|VOTER_NAME       |\n",
      "+-----------------+\n",
      "|the final 201... |\n",
      "|Tiffinni A. Young|\n",
      "|Monica R. Alonzo |\n",
      "|Kevin Felder     |\n",
      "|Mark Clayton     |\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# File Path\n",
    "file_path = \".../data/datacamp/voter/\"\n",
    "\n",
    "voter_df = spark.read.parquet(file_path)\n",
    "\n",
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select(\"VOTER_NAME\").distinct().show(5, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains(\"_\"))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select(\"VOTER_NAME\").distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying DataFrame columns\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - first_name and last_name. She asks you to split the VOTER_NAME column into words on any space character. You'll treat the last word as the last_name, and all other words as the first_name. You'll be using some new functions in this exercise including .split(), .size(), and .getItem(). The .getItem(index) takes an integer value to return the appropriately numbered item in the column. The functions .split() and .size() are in the pyspark.sql.functions library.\n",
    "\n",
    "Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+----------+---------+\n",
      "|      DATE|        TITLE|     VOTER_NAME|first_name|last_name|\n",
      "+----------+-------------+---------------+----------+---------+\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|\n",
      "+----------+-------------+---------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when() example\n",
    "The when() clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our voter_df DataFrame to add a random number to any voting member that is defined as a \"Councilmember\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|     VOTER_NAME|first_name|last_name|         random_val|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.5088622853656986|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.22711379138967347|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.7076059242271496|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.9501812886138689|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.9778281065662836|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == \"Councilmember\", F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When / Otherwise\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your voter_df DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|     VOTER_NAME|first_name|last_name|         random_val|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|  0.740034089413097|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.18192571641220723|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.7778368240852591|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.49766880191751284|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.28931653273216695|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------+-----------------+----------+---------+----------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|first_name|last_name|random_val|\n",
      "+----------+-------------+-----------------+----------+---------+----------+\n",
      "|09/05/2018|Mayor Pro Tem|     Casey Thomas|     Casey|   Thomas|       0.0|\n",
      "|08/09/2017|Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/09/2017|Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/09/2017|Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "|08/09/2017|Mayor Pro Tem|Dwaine R. Caraway|    Dwaine|  Caraway|       0.0|\n",
      "+----------+-------------+-----------------+----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', F.rand()) \\\n",
    "                               .when(voter_df.TITLE == \"Mayor\", 2) \\\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show(5)\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using user defined functions in Spark\n",
    "You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "For this exercise, we'll use our voter_df DataFrame, but you're going to replace the first_name column with the first and middle names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------------+----------+---------+-------------------+-------------------+---------------------+\n",
      "|      DATE|        TITLE|     VOTER_NAME|first_name|last_name|         random_val|             splits|first_and_middle_name|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+-------------------+---------------------+\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|  0.740034089413097|[B., Adam, McGough]|              B. Adam|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.18192571641220723|[B., Adam, McGough]|              B. Adam|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough| 0.7778368240852591|[B., Adam, McGough]|              B. Adam|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.49766880191751284|[B., Adam, McGough]|              B. Adam|\n",
      "|09/28/2016|Councilmember|B. Adam McGough|        B.|  McGough|0.28931653273216695|[B., Adam, McGough]|              B. Adam|\n",
      "+----------+-------------+---------------+----------+---------+-------------------+-------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+')) \n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an ID Field\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 33 rows in the voter_df DataFrame.\n",
      "\n",
      "+-------------------+--------------+\n",
      "|          VOTERNAME|        ROW_ID|\n",
      "+-------------------+--------------+\n",
      "|    Lee M. Kleinman|40742059769856|\n",
      "| Rickey D. Callahan|40638980554752|\n",
      "| Philip T. Kingston|38920993636352|\n",
      "|      Casey  Thomas|38147899523072|\n",
      "|Carolyn King Arnold|37709812858880|\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Path\n",
    "file_path = \".../data/datacamp/votes\"\n",
    "\n",
    "# Load in data\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTERNAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 5000 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 1 partitions in the voter_df_single DataFrame.\n",
      "\n",
      "+-------------------+--------------+\n",
      "|          VOTERNAME|        ROW_ID|\n",
      "+-------------------+--------------+\n",
      "|    Lee M. Kleinman|40742059769856|\n",
      "| Rickey D. Callahan|40638980554752|\n",
      "| Philip T. Kingston|38920993636352|\n",
      "|      Casey  Thomas|38147899523072|\n",
      "|Carolyn King Arnold|37709812858880|\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+------+\n",
      "|          VOTERNAME|ROW_ID|\n",
      "+-------------------+------+\n",
      "|    Lee M. Kleinman|    32|\n",
      "| the  final   20...|    31|\n",
      "|Rickey D.  Callahan|    30|\n",
      "|      Casey  Thomas|    29|\n",
      "|       Casey Thomas|    28|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repartition\n",
    "voter_df_single = voter_df.repartition(1)\n",
    "\n",
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(5)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the drastic difference in the 'ROW_ID' values between the two Data Frames. Understanding how lazy processing and partitioning behave are integral to mastering Spark. Make sure to always test your assumptions when creating a Spark workflow to avoid nasty suprises in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More ID tricks\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|      ROW_ID|\n",
      "+------------+\n",
      "|  8589934592|\n",
      "| 34359738368|\n",
      "| 42949672960|\n",
      "| 51539607552|\n",
      "|103079215104|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+\n",
      "|          ROW_ID|\n",
      "+----------------+\n",
      "|9.62072674304E11|\n",
      "|9.62072674305E11|\n",
      "|9.62072674306E11|\n",
      "|9.62072674307E11|\n",
      "|9.62072674308E11|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Path\n",
    "file_path = \".../data/datacamp/voter_month/\"\n",
    "\n",
    "# Load in data\n",
    "voter_df_march = spark.read.csv(file_path + 'voter_march.csv', header=True)\n",
    "voter_df_april = spark.read.csv(file_path + 'voter_april.csv', header=True)\n",
    "\n",
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show(5)\n",
    "voter_df_april.select('ROW_ID').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic work! It's easy to forget that the output of a Spark method can often be modified before being assigned. This provides a lot of power and flexibility, especially when trying to migrate tasks from various technologies. Consider how you could use everything we've learned in this chapter to create a combination ID containing a name, a new ID, and perhaps a conditional value. When you are able to view your tasks as compositions of available functions, you can clean and modify your data in any way you see fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:playground_py36]",
   "language": "python",
   "name": "conda-env-playground_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
