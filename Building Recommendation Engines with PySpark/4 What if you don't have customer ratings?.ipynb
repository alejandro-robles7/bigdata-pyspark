{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if you don't have customer ratings?\n",
    "In most real-life situations, you won't not have \"perfect\" customer data available to build an ALS model. This chapter will teach you how to use your customer behavior data to \"infer\" customer ratings and use those inferred ratings to build an ALS recommendation engine. Using the Million Songs Dataset as well as another version of the MovieLens dataset, this chapter will show you how to use the data available to you to build a recommendation engine using ALS and evaluate it's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSD summary statistics\n",
    "Let's get familiar with the Million Songs Echo Nest Taste Profile data subset. For purposes of this course, we'll just call it the Million Songs dataset or msd. Let's get the number of users and the number of songs. Let's also see which songs have the most plays from this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path\n",
    "file_path = \".../data/datacamp/\"\n",
    "\n",
    "# Load data\n",
    "msd = spark.read.parquet(file_path + 'msd')\n",
    "\n",
    "# Look at the data\n",
    "msd.show(5)\n",
    "\n",
    "# Count the number of distinct userIds\n",
    "user_count = msd.select(\"userId\").distinct().count()\n",
    "print(\"Number of users: \", user_count)\n",
    "\n",
    "# Count the number of distinct songIds\n",
    "song_count = msd.select(\"songId\").distinct().count()\n",
    "print(\"Number of songs: \", song_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped summary statistics\n",
    "In this exercise, we are going to combine the .groupBy() and .filter() methods that you've used previously to calculate the min() and avg() number of users that have rated each song, and the min() and avg() number of songs that each user has rated.\n",
    "\n",
    "Because our data now includes 0's for items not yet consumed, we'll need to .filter() them out when doing grouped summary statistics like this. The msd dataset is provided for you here. The col(), min(), and avg() functions from pyspark.sql.functions have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, avg\n",
    "\n",
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings from users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. Users have at least 21 implicit ratings with an average of 77 and each song has at least 3 implicit ratings with an average of 35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Zeros\n",
    "Many recommendation engines use implicit ratings. In many cases these datasets don't include behavior counts for items that a user has never purchased. In these cases, you'll need to add them and include zeros. The dataframe Z is provided for you. It contains userId's, productId's and num_purchases which is the number of times a user has purchased a specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "Z = spark.createDataFrame(DataFrame({'num_purchases': [1, 23, 9, 2, 5, 21, 8, 96],\n",
    " 'productId': [777, 44, 227, 1981, 2390, 1662, 1492, 1811],\n",
    " 'userId': [2112, 7, 1132, 686, 42, 13, 2112, 22]})).repartition(30)\n",
    "\n",
    "# View the data\n",
    "Z.show(5)\n",
    "\n",
    "# Extract distinct userIds and productIds\n",
    "users = Z.select(\"userId\").distinct()\n",
    "products = Z.select(\"productId\").distinct()\n",
    "\n",
    "# Cross join users and products\n",
    "cj = users.crossJoin(products)\n",
    "\n",
    "# Join cj and Z\n",
    "Z_expanded = cj.join(Z, [\"userId\", \"productId\"], \"left\").fillna(0)\n",
    "\n",
    "# View Z_expanded\n",
    "Z_expanded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify ALS Hyperparameters\n",
    "You're now going to build your first implicit rating recommendation engine using ALS. To do this, you will first tell Spark what values you want it to try when finding the best model.\n",
    "\n",
    "Four empty lists are provided below. You will fill them with specific values that Spark can use to build several different ALS models. In the next exercise, you'll tell Spark to build out these models using the lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the lists below\n",
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Implicit Models\n",
    "Now that you have all of your hyperparameter values specified, let's have Spark build enough models to test each combination. To facilitate this, a for loop is provided here. Follow the instructions below to automatically create these ALS models. In subsequent exercises you will run these models on test datasets to see which one performs the best.\n",
    "\n",
    "The ALS algorithm is already imported for you. The lists you created in the last exercise (ranks, maxIters, regParams, alphas) have been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"userId\", itemCol= \"songId\", ratingCol= \"num_plays\", rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Cross-Validated Implicit ALS Model\n",
    "Now that we have several ALS models, each with a different set of hyperparameter values, we can train them on a training portion of the msd dataset using cross validation, and then run them on a test set of data and evaluate how well each one performs using the ROEM function discussed earlier. Unfortunately, this takes too much time for this exercise, so it has been done separately. But for your reference you can evaluate your model_list using the following loop (we are using the msd dataset in this case):\n",
    "\n",
    "```python\n",
    "# Split the data into training and test sets\n",
    "(training, test) = msd.randomSplit([0.8, 0.2])\n",
    "\n",
    "#Building 5 folds within the training set.\n",
    "train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "# Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "# Loops through all models and all folds\n",
    "for model in model_list:\n",
    "    for ft_pair in foldlist:\n",
    "\n",
    "        # Fits model to fold within training data\n",
    "        fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "        # Generates predictions using fitted_model on respective CV test data\n",
    "        predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "        # Generates and prints a ROEM metric CV test data\n",
    "        r = ROEM(predictions)\n",
    "        print (\"ROEM: \", r)\n",
    "\n",
    "    # Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(training)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "    # Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM: \", v_ROEM)\n",
    "```\n",
    "For purposes of walking you through the steps, the test predictions for 192 models have already been generated, and their ROEM has been calculated. They are found in the ROEMS list provided. Because a list isn't unique to Pyspark, and because numpy works really well with lists, we're going to use numpy here. Follow the instructions below to find the best ROEM and the model that provided it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy\n",
    "\n",
    "# Find the index of the smallest ROEM\n",
    "i = numpy.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", i)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Parameters\n",
    "You've now tested 192 different models on the msd dataset, and you found the best ROEM and its respective model (model 38).\n",
    "\n",
    "You now need to extract the hyperparameters. The model_list you created previously is provided here. It contains all 192 models you generated. Use the instructions below to extract the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best_model\n",
    "best_model = model_list[38]\n",
    "\n",
    "# Extract the Rank\n",
    "print (\"Rank: \", best_model.getRank())\n",
    "\n",
    "# Extract the MaxIter value\n",
    "print (\"MaxIter: \", best_model.getMaxIter())\n",
    "\n",
    "# Extract the RegParam value\n",
    "print (\"RegParam: \", best_model.getRegParam())\n",
    "\n",
    "# Extract the Alpha value\n",
    "print (\"Alpha: \", best_model.getAlpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. Looks like a low rank, a higher maxIter, a low regParam, and a medium-high alpha is keeping the ROEM low. Because some of these values are on the high and low ends of the values we tried, it would be worth adding some additional values to test in our hyperparameter values, and doing this step again. But for right now, you should understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Model Performance\n",
    "You've already built several ALS models, so we won't do that again. An implicit ALS model has already been fitted to the binary ratings of the MovieLens dataset. Let's look at the binary_test_predictions from this model to see what we can learn.\n",
    "\n",
    "The ROEM() function has been defined for you. Feel free to run help(ROEM) in the console if you want more details on how to execute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROEM(predictions, default_df = binary_test_predictions.toPandas()):\n",
    "  \"\"\" \n",
    "    Calculates the Rank Order Error Metric (ROEM) for a set of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "      - predictions: pyspark.sql.dataframe DataFrame object with the following columns:\n",
    "        *Unique user ID\n",
    "        *Unique item ID\n",
    "        *Consumed or Viewed: where a 1 indicates that an item has been consumed and 0 indicates that it has not been consumed\n",
    "        *Prediction which is a measure of confident the model is that the user prefers the relevant item\n",
    "      \n",
    "    Returns: A decimal number representing the ROEM. The lower the number the better the predictions.\n",
    "    \"\"\"\n",
    "  preds_pd = predictions.toPandas()\n",
    "  \n",
    "  if preds_pd.equals(binary_test_predictions):\n",
    "    print (\"ROEM: 0.07436376290899886\")\n",
    "  else: \n",
    "    # Creates table that can be queried\n",
    "    predictions.createOrReplaceTempView(\"predictions\") \n",
    "    # Sum of total number of plays of all songs\n",
    "    denominator = predictions.groupBy().sum(\"viewed\").collect()[0][0]\n",
    "    # Calculating rankings of songs predictions by user\n",
    "    spark.sql(\"SELECT userID, viewed, PERCENT_RANK() OVER (PARTITION BY userId ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "    # Multiplies the rank of each song by the number of plays and adds the products together\n",
    "    numerator = spark.sql('SELECT SUM(viewed * rank) FROM rankings').collect()[0][0]\n",
    "    result = numerator/denominator\n",
    "    print (\"ROEM:\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the col function\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Look at the test predictions\n",
    "binary_test_predictions.show()\n",
    "\n",
    "# Evaluate ROEM on test predictions\n",
    "ROEM(binary_test_predictions)\n",
    "\n",
    "# Look at user 42's test predictions\n",
    "binary_test_predictions.filter(col(\"userId\") == 42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job. The model has a pretty low ROEM. Did you notice that the model predicted some high numbers for unseen movies? This indicates that the model is creating recommendations from the movies that users have not seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations From Binary Data\n",
    "So you see from the ROEM, these models can still generate meaningful test predictions. Let's look at the actual recommendations now.\n",
    "\n",
    "The col function from the pyspark.sql.functions class has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View user 26's original ratings\n",
    "print (\"User 26 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 26's recommendations\n",
    "print (\"User 26 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 99's original ratings\n",
    "print (\"User 99 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 99).show()\n",
    "\n",
    "# View user 99's recommendations\n",
    "print (\"User 99 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 99).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. ALS seems to have picked up on the fact that user 26 likes thrillers, crime movies, action and adventure, and that user 99 likes dramas and romances. Do these look like good recommendations to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers\n",
    "- https://github.com/jamenlong/ALS_expected_percent_rank_cv/blob/master/wide_to_long_function.py\n",
    "- http://yifanhu.net/PUB/cf.pdf\n",
    "- https://github.com/jamenlong/ALS_expected_percent_rank_cv/blob/master/ROEM_cv.py\n",
    "- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.4684&rep=rep1&type=pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:playground_py36]",
   "language": "python",
   "name": "conda-env-playground_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
