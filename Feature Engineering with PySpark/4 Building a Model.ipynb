{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "\n",
    "In this chapter we'll learn how to choose which type of model we want. Then we will learn how to apply our data to the model and evaluate it. Lastly, we'll learn how to interpret the results and save the model for later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Time Splits\n",
    "In the video, we learned why splitting data randomly can be dangerous for time series as data from the future can cause overfitting in our model. Often with time series, you acquire new data as it is made available and you will want to retrain your model using the newest data. In the video, we showed how to do a percentage split for test and training sets but suppose you wish to train on all available data except for the last 45days which you want to use for a test set.\n",
    "\n",
    "In this exercise, we will create a function to find the split date for using the last 45 days of data for testing and the rest for training. Please note that timedelta() has already been imported for you from the standard python library datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://qa.dssa.thetradedesk.com/libs/playground_py36.zip to ../libs/playground_py36.zip\n",
      "No existing SparkSession\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://qa.dssa.thetradedesk.com/libs/playground_py36.zip ../libs/playground_py36.zip\n",
    "\n",
    "from utils.start_session import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from pyspark.sql.functions import to_date, substring_index, expr\n",
    "\n",
    "# File Path\n",
    "file_path = \"s3a://qa.dssa.thetradedesk.com/data/datacamp/\"\n",
    "\n",
    "# Read the file into a dataframe\n",
    "df = spark.read.parquet(file_path + 'real_estate') \\\n",
    "    .select('DAYSONMARKET', 'LISTDATE') \\\n",
    "    .withColumn('LISTDATE', substring_index(\"LISTDATE\", \" \", 1)) \\\n",
    "    .withColumn('LISTDATE', to_date('LISTDATE', 'M/dd/yy')) \\\n",
    "    .withColumn(\"OFFMKTDATE\", expr(\"date_add(LISTDATE,DAYSONMARKET)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "  # Find how many days our data spans\n",
    "  max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
    "  min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
    "  # Subtract an integer number of days from the last date in dataset\n",
    "  split_date = max_date - timedelta(days=test_days)\n",
    "  return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, 'OFFMKTDATE')\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df['OFFMKTDATE'] < split_date) \n",
    "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. Creating functions like this take more time upfront but if you intend to use the model over and over again its worth spending more time to do thing properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Time Features\n",
    "We have mentioned throughout this course some of the dangers of leaking information to your model during training. Data leakage will cause your model to have very optimistic metrics for accuracy but once real data is run through it the results are often very disappointing.\n",
    "\n",
    "In this exercise, we are going to ensure that DAYSONMARKET only reflects what information we have at the time of predicting the value. I.e., if the house is still on the market, we don't know how many more days it will stay on the market. We need to adjust our test_df to reflect what information we currently have as of 2017-12-10.\n",
    "\n",
    "NOTE: This example will use the lit() function. This function is used to allow single values where an entire column is expected in a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------------+------------+\n",
      "|  LISTDATE|OFFMKTDATE|DAYSONMARKET_Original|DAYSONMARKET|\n",
      "+----------+----------+---------------------+------------+\n",
      "|2017-12-07|2017-12-23|                   16|           3|\n",
      "|2017-11-15|2017-12-11|                   26|          25|\n",
      "|2017-11-13|2017-12-11|                   28|          27|\n",
      "|2017-07-14|2017-12-19|                  158|         149|\n",
      "|2017-10-19|2018-01-06|                   79|          52|\n",
      "+----------+----------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('2017-12-10'))\n",
    "# Create Sequential Test set\n",
    "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date)\n",
    "\n",
    "# Create a copy of DAYSONMARKET to review later\n",
    "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
    "\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn('DAYSONMARKET', datediff(split_date, 'LISTDATE'))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'OFFMKTDATE', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done. Thinking critically about what information would be available at the time of prediction is crucial in having accurate model metrics and saves a lot of embarassment down the road if decisions are being made based off your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering For Random Forests\n",
    "Considering what steps you'll need to take to preprocess your data before running a machine learning algorithm is important or you could get invalid results. Which of the following preprocessing techniques are needed for Random Forest Regression?\n",
    "\n",
    "1. Perform value replacement for missing values and encode categorical text features to numeric.\n",
    "2. Scale all features between 0 and 1 with a min max scaler.\n",
    "3. Ensure all variables are standard normal distributed, mean 0 and standard deviation of 1.\n",
    "\n",
    "### Answer is (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct. Missing values are handled by Random Forests internally where they partition on missing values. As long as you replace them with something outside of the range of normal values, they will be handled correctly. Likewise, categorical features only need to be mapped to numbers, they are fine to stay all in one column by using a StringIndexer as we saw in chapter 3. OneHot encoding which converts each possible value to its own boolean feature is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Columns with Low Observations\n",
    "After doing a lot of feature engineering it's a good idea to take a step back and look at what you've created. If you've used some automation techniques on your categorical features like exploding or OneHot Encoding you may find that you now have hundreds of new binary features. While the subject of feature selection is material for a whole other course but there are some quick steps you can take to reduce the dimensionality of your data set.\n",
    "\n",
    "In this exercise, we are going to remove columns that have less than 30 observations. 30 is a common minimum number of observations for statistical significance. Any less than that and the relationships cause overfitting because of a sheer coincidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 500 Columns: 253\n",
      "Rows: 500 Columns: 245\n"
     ]
    }
   ],
   "source": [
    "# Read the file into a dataframe\n",
    "df = spark.read.csv(file_path + 'features/sample_df.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Binary Columns\n",
    "binary_cols = ['FENCE_WIRE', 'FENCE_ELECTRIC', 'FENCE_NAN', 'FENCE_PARTIAL', 'FENCE_RAIL', 'FENCE_OTHER', 'FENCE_CHAIN LINK', 'FENCE_FULL', 'FENCE_NONE', 'FENCE_PRIVACY', 'FENCE_WOOD', 'FENCE_INVISIBLE', 'ROOF_ASPHALT SHINGLES', 'ROOF_SHAKES', 'ROOF_NAN', 'ROOF_UNSPECIFIED SHINGLE', 'ROOF_SLATE', 'ROOF_PITCHED', 'ROOF_FLAT', 'ROOF_TAR/GRAVEL', 'ROOF_OTHER', 'ROOF_METAL', 'ROOF_TILE', 'ROOF_RUBBER', 'ROOF_WOOD SHINGLES', 'ROOF_AGE OVER 8 YEARS', 'ROOF_AGE 8 YEARS OR LESS', 'POOLDESCRIPTION_NAN', 'POOLDESCRIPTION_HEATED', 'POOLDESCRIPTION_NONE', 'POOLDESCRIPTION_SHARED', 'POOLDESCRIPTION_INDOOR', 'POOLDESCRIPTION_OUTDOOR', 'POOLDESCRIPTION_ABOVE GROUND', 'POOLDESCRIPTION_BELOW GROUND', 'GARAGEDESCRIPTION_ASSIGNED', 'GARAGEDESCRIPTION_TANDEM', 'GARAGEDESCRIPTION_UNCOVERED/OPEN', 'GARAGEDESCRIPTION_TUCKUNDER', 'GARAGEDESCRIPTION_DRIVEWAY - ASPHALT', 'GARAGEDESCRIPTION_HEATED GARAGE', 'GARAGEDESCRIPTION_UNDERGROUND GARAGE', 'GARAGEDESCRIPTION_DRIVEWAY - SHARED', 'GARAGEDESCRIPTION_CONTRACT PKG REQUIRED', 'GARAGEDESCRIPTION_GARAGE DOOR OPENER', 'GARAGEDESCRIPTION_MORE PARKING OFFSITE FOR FEE', 'GARAGEDESCRIPTION_VALET PARKING FOR FEE', 'GARAGEDESCRIPTION_OTHER', 'GARAGEDESCRIPTION_MORE PARKING ONSITE FOR FEE', 'GARAGEDESCRIPTION_DRIVEWAY - OTHER SURFACE', 'GARAGEDESCRIPTION_DETACHED GARAGE', 'GARAGEDESCRIPTION_SECURED', 'GARAGEDESCRIPTION_CARPORT', 'GARAGEDESCRIPTION_DRIVEWAY - CONCRETE', 'GARAGEDESCRIPTION_ON-STREET PARKING ONLY', 'GARAGEDESCRIPTION_COVERED', 'GARAGEDESCRIPTION_INSULATED GARAGE', 'GARAGEDESCRIPTION_UNASSIGNED', 'GARAGEDESCRIPTION_NONE', 'GARAGEDESCRIPTION_DRIVEWAY - GRAVEL', 'GARAGEDESCRIPTION_NO INT ACCESS TO DWELLING', 'GARAGEDESCRIPTION_UNITS VARY', 'GARAGEDESCRIPTION_ATTACHED GARAGE', 'APPLIANCES_NAN', 'APPLIANCES_COOKTOP', 'APPLIANCES_WALL OVEN', 'APPLIANCES_WATER SOFTENER - OWNED', 'APPLIANCES_DISPOSAL', 'APPLIANCES_DISHWASHER', 'APPLIANCES_OTHER', 'APPLIANCES_INDOOR GRILL', 'APPLIANCES_WASHER', 'APPLIANCES_RANGE', 'APPLIANCES_REFRIGERATOR', 'APPLIANCES_FURNACE HUMIDIFIER', 'APPLIANCES_TANKLESS WATER  HEATER', 'APPLIANCES_ELECTRONIC AIR FILTER', 'APPLIANCES_MICROWAVE', 'APPLIANCES_EXHAUST FAN/HOOD', 'APPLIANCES_NONE', 'APPLIANCES_CENTRAL VACUUM', 'APPLIANCES_TRASH COMPACTOR', 'APPLIANCES_AIR-TO-AIR EXCHANGER', 'APPLIANCES_DRYER', 'APPLIANCES_FREEZER', 'APPLIANCES_WATER SOFTENER - RENTED', 'EXTERIOR_SHAKES', 'EXTERIOR_CEMENT BOARD', 'EXTERIOR_BLOCK', 'EXTERIOR_VINYL', 'EXTERIOR_FIBER BOARD', 'EXTERIOR_OTHER', 'EXTERIOR_METAL', 'EXTERIOR_BRICK/STONE', 'EXTERIOR_STUCCO', 'EXTERIOR_ENGINEERED WOOD', 'EXTERIOR_WOOD', 'DININGROOMDESCRIPTION_EAT IN KITCHEN', 'DININGROOMDESCRIPTION_NAN', 'DININGROOMDESCRIPTION_OTHER', 'DININGROOMDESCRIPTION_LIVING/DINING ROOM', 'DININGROOMDESCRIPTION_SEPARATE/FORMAL DINING ROOM', 'DININGROOMDESCRIPTION_KITCHEN/DINING ROOM', 'DININGROOMDESCRIPTION_INFORMAL DINING ROOM', 'DININGROOMDESCRIPTION_BREAKFAST AREA', 'BASEMENT_FINISHED (LIVABLE)', 'BASEMENT_PARTIAL', 'BASEMENT_SUMP PUMP', 'BASEMENT_INSULATING CONCRETE FORMS', 'BASEMENT_CRAWL SPACE', 'BASEMENT_PARTIAL FINISHED', 'BASEMENT_CONCRETE BLOCK', 'BASEMENT_DRAINAGE SYSTEM', 'BASEMENT_POURED CONCRETE', 'BASEMENT_UNFINISHED', 'BASEMENT_DRAIN TILED', 'BASEMENT_WOOD', 'BASEMENT_FULL', 'BASEMENT_EGRESS WINDOWS', 'BASEMENT_DAY/LOOKOUT WINDOWS', 'BASEMENT_SLAB', 'BASEMENT_STONE', 'BASEMENT_NONE', 'BASEMENT_WALKOUT', 'BATHDESC_MAIN FLOOR 1/2 BATH', 'BATHDESC_TWO MASTER BATHS', 'BATHDESC_MASTER WALK-THRU', 'BATHDESC_WHIRLPOOL', 'BATHDESC_NAN', 'BATHDESC_3/4 BASEMENT', 'BATHDESC_TWO BASEMENT BATHS', 'BATHDESC_OTHER', 'BATHDESC_3/4 MASTER', 'BATHDESC_MAIN FLOOR 3/4 BATH', 'BATHDESC_FULL MASTER', 'BATHDESC_MAIN FLOOR FULL BATH', 'BATHDESC_WALK-IN SHOWER', 'BATHDESC_SEPARATE TUB & SHOWER', 'BATHDESC_FULL BASEMENT', 'BATHDESC_BASEMENT', 'BATHDESC_WALK THRU', 'BATHDESC_BATHROOM ENSUITE', 'BATHDESC_PRIVATE MASTER', 'BATHDESC_JACK & JILL 3/4', 'BATHDESC_UPPER LEVEL 1/2 BATH', 'BATHDESC_ROUGH IN', 'BATHDESC_UPPER LEVEL FULL BATH', 'BATHDESC_1/2 MASTER', 'BATHDESC_1/2 BASEMENT', 'BATHDESC_JACK AND JILL', 'BATHDESC_UPPER LEVEL 3/4 BATH', 'ZONING_INDUSTRIAL', 'ZONING_BUSINESS/COMMERCIAL', 'ZONING_OTHER', 'ZONING_RESIDENTIAL-SINGLE', 'ZONING_RESIDENTIAL-MULTI-FAMILY', 'COOLINGDESCRIPTION_WINDOW', 'COOLINGDESCRIPTION_WALL', 'COOLINGDESCRIPTION_DUCTLESS MINI-SPLIT', 'COOLINGDESCRIPTION_NONE', 'COOLINGDESCRIPTION_GEOTHERMAL', 'COOLINGDESCRIPTION_CENTRAL', 'CITY:LELM - LAKE ELMO', 'CITY:MAPW - MAPLEWOOD', 'CITY:OAKD - OAKDALE', 'CITY:STP - SAINT PAUL', 'CITY:WB - WOODBURY', 'LISTTYPE:EXCLUSIVE AGENCY', 'LISTTYPE:EXCLUSIVE RIGHT', 'LISTTYPE:EXCLUSIVE RIGHT WITH EXCLUSIONS', 'LISTTYPE:OTHER', 'LISTTYPE:SERVICE AGREEMENT', 'SCHOOLDISTRICTNUMBER:6 - SOUTH ST. PAUL', 'SCHOOLDISTRICTNUMBER:622 - NORTH ST PAUL-MAPLEWOOD', 'SCHOOLDISTRICTNUMBER:623 - ROSEVILLE', 'SCHOOLDISTRICTNUMBER:624 - WHITE BEAR LAKE', 'SCHOOLDISTRICTNUMBER:625 - ST. PAUL', 'SCHOOLDISTRICTNUMBER:832 - MAHTOMEDI', 'SCHOOLDISTRICTNUMBER:833 - SOUTH WASHINGTON COUNTY', 'SCHOOLDISTRICTNUMBER:834 - STILLWATER', 'POTENTIALSHORTSALE:NO', 'POTENTIALSHORTSALE:NOT DISCLOSED', 'STYLE:(CC) CONVERTED MANSION', 'STYLE:(CC) HIGH RISE (4+ LEVELS)', 'STYLE:(CC) LOW RISE (3- LEVELS)', 'STYLE:(CC) MANOR/VILLAGE', 'STYLE:(CC) TWO UNIT', 'STYLE:(SF) FOUR OR MORE LEVEL SPLIT', 'STYLE:(SF) MODIFIED TWO STORY', 'STYLE:(SF) MORE THAN TWO STORIES', 'STYLE:(SF) ONE 1/2 STORIES', 'STYLE:(SF) ONE STORY', 'STYLE:(SF) OTHER', 'STYLE:(SF) SPLIT ENTRY (BI-LEVEL)', 'STYLE:(SF) THREE LEVEL SPLIT', 'STYLE:(SF) TWO STORIES', 'STYLE:(TH) DETACHED', 'STYLE:(TH) QUAD/4 CORNERS', 'STYLE:(TH) SIDE X SIDE', 'STYLE:(TW) TWIN HOME', 'ASSUMABLEMORTGAGE:INFORMATION COMING', 'ASSUMABLEMORTGAGE:NOT ASSUMABLE', 'ASSUMABLEMORTGAGE:YES W/ QUALIFYING', 'ASSUMABLEMORTGAGE:YES W/NO QUALIFYING', 'ASSESSMENTPENDING:NO', 'ASSESSMENTPENDING:UNKNOWN', 'ASSESSMENTPENDING:YES']\n",
    "\n",
    "obs_threshold = 30\n",
    "cols_to_remove = list()\n",
    "# Inspect first 10 binary columns in list\n",
    "for col in binary_cols[0:10]:\n",
    "  # Count the number of 1 values in the binary column\n",
    "  obs_count = df.agg({col: 'sum'}).collect()[0][0]\n",
    "  # If less than our observation threshold, remove\n",
    "  if obs_count < obs_threshold:\n",
    "    cols_to_remove.append(col)\n",
    "    \n",
    "# Drop columns and print starting and ending dataframe shapes\n",
    "new_df = df.drop(*cols_to_remove)\n",
    "\n",
    "print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
    "print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing low observation features is helpful in many ways. It can improve processing speed of model training, prevent overfitting by coincidence and help interpretability by reducing the number of things to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naively Handling Missing and Categorical Values\n",
    "Random Forest Regression is robust enough to allow us to ignore many of the more time consuming and tedious data preparation steps. While some implementations of Random Forest handle missing and categorical values automatically, PySpark's does not. The math remains the same however so we can get away with some naive value replacements.\n",
    "\n",
    "For missing values since our data is strictly positive, we will assign -1. The random forest will split on this value and handle it differently than the rest of the values in the same feature.\n",
    "\n",
    "For categorical values, we can just map the text values to numbers and again the random forest will appropriately handle them by splitting on them. In this example, we will dust off pipelines from Introduction to PySpark to write our code more concisely. Please note that the exercise will start by displaying the dtypes of the columns in the dataframe, compare them to the results at the end of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CITY', 'string'), ('LISTTYPE', 'string'), ('SCHOOLDISTRICTNUMBER', 'string'), ('POTENTIALSHORTSALE', 'string'), ('STYLE', 'string'), ('ASSUMABLEMORTGAGE', 'string'), ('ASSESSMENTPENDING', 'string'), ('WALKSCORE', 'double'), ('BIKESCORE', 'double')]\n",
      "[('WALKSCORE', 'double'), ('BIKESCORE', 'double'), ('CITY_IDX', 'double'), ('LISTTYPE_IDX', 'double'), ('SCHOOLDISTRICTNUMBER_IDX', 'double'), ('POTENTIALSHORTSALE_IDX', 'double'), ('STYLE_IDX', 'double'), ('ASSUMABLEMORTGAGE_IDX', 'double'), ('ASSESSMENTPENDING_IDX', 'double')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# Read the file into a dataframe\n",
    "df = spark.read.csv(file_path + 'features/randomforest.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Categorical Columns\n",
    "categorical_cols = ['CITY', 'LISTTYPE', 'SCHOOLDISTRICTNUMBER', 'POTENTIALSHORTSALE', 'STYLE', 'ASSUMABLEMORTGAGE', 'ASSESSMENTPENDING']\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "# Replace missing values\n",
    "df = df.fillna(-1, subset=['WALKSCORE', 'BIKESCORE'])\n",
    "\n",
    "# Create list of StringIndexers using list comprehension\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "# Create pipeline of indexers\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "# Fit and Transform the pipeline to the original data\n",
    "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Clean up redundant columns\n",
    "df_indexed = df_indexed.drop(*categorical_cols)\n",
    "# Inspect data transformations\n",
    "print(df_indexed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can hopefully see, handling missing and categorical values for Random Forest Regression is fairly painless compared to some of the other things we would have had to do if we chose a different algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression Model\n",
    "One of the great things about PySpark ML module is that most algorithms can be tried and tested without changing much code. Random Forest Regression is a fairly simple ensemble model, using bagging to fit. Another tree based ensemble model is Gradient Boosted Trees which uses a different approach called boosting to fit. In this exercise let's train a GBTRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe rows: 4828\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Read the file into a dataframe\n",
    "df = spark.read.csv(file_path + 'features/gbt_train.csv', header=True, inferSchema=True).repartition(300)\n",
    "\n",
    "print(\"Dataframe rows:\", df.count())\n",
    "\n",
    "# Cast to double\n",
    "df = df.withColumn('TAX_TO_LIST', df['TAX_TO_LIST'].cast(DoubleType()))\n",
    "\n",
    "# Columns to vectorize\n",
    "col2vectorize = df.columns\n",
    "col2vectorize.remove(\"SALESCLOSEPRICE\")\n",
    "\n",
    "# Vectorize features\n",
    "train_df = VectorAssembler(\n",
    "    inputCols=col2vectorize,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\") \\\n",
    "    .transform(df) \\\n",
    "    .select(\"features\", \"SALESCLOSEPRICE\")\n",
    "\n",
    "\n",
    "# Train a Gradient Boosted Trees (GBT) model.\n",
    "gbt = GBTRegressor(featuresCol='features',\n",
    "                           labelCol='SALESCLOSEPRICE',\n",
    "                           predictionCol=\"Prediction_Price\",\n",
    "                           seed=42\n",
    "                           )\n",
    "\n",
    "# Train model.\n",
    "model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Initialize model with columns to utilize\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",\n",
    "                            labelCol=\"SALESCLOSEPRICE\",\n",
    "                            predictionCol=\"Prediction_Price\",\n",
    "                            seed=42\n",
    "                    )\n",
    "\n",
    "# Train model\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating & Comparing Algorithms\n",
    "Now that we've created a new model with GBTRegressor its time to compare it against our baseline of RandomForestRegressor. To do this we will compare the predictions of both models to the actual data and calculate RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a dataframe\n",
    "df = spark.read.csv(file_path + 'features/gbt_test.csv', header=True, inferSchema=True).repartition(300)\n",
    "\n",
    "# Cast to double\n",
    "df = df.withColumn('TAX_TO_LIST', df['TAX_TO_LIST'].cast(DoubleType()))\n",
    "\n",
    "# Columns to vectorize\n",
    "col2vectorize = df.columns\n",
    "col2vectorize.remove(\"SALESCLOSEPRICE\")\n",
    "\n",
    "# Vectorize features\n",
    "test_df = VectorAssembler(\n",
    "    inputCols=col2vectorize,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\") \\\n",
    "    .transform(df) \\\n",
    "    .select(\"features\", \"SALESCLOSEPRICE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with Gradient Boosted Trees \n",
    "gbt_predictions = model.transform(test_df)\n",
    "\n",
    "# Make predictions with Random Forest\n",
    "rfr_predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees RMSE: 74227.95561831584\n",
      "Gradient Boosted Trees R^2: 0.5889591121645428\n",
      "Random Forest Regression RMSE: 17536.627881953387\n",
      "Random Forest Regression R^2: 0.9770574229279423\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", \n",
    "                                predictionCol=\"Prediction_Price\")\n",
    "# Dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "for key, preds in models.items():\n",
    "  # Create evaluation metrics\n",
    "  rmse = evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"})\n",
    "  r2 = evaluator.evaluate(preds, {evaluator.metricName: \"r2\"})\n",
    "  \n",
    "  # Print Model Metrics\n",
    "  print(key + ' RMSE: ' + str(rmse))\n",
    "  print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful in discarding algorithms just because its first pass was not great. Even though Gradient Boosted Trees performed much worse it has many hyper parameters, with proper tuning it would have comparable or better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results\n",
    "It is almost always important to know which features are influencing your prediction the most. Perhaps its counterintuitive and that's an insight? Perhaps a hand full of features account for most of the accuracy of your model and you don't need to perform time acquiring or massaging other features.\n",
    "\n",
    "In this example we will be looking at a model that has been trained without any LISTPRICE information. With that gone, what influences the price the most?\n",
    "\n",
    "NOTE: The array of feature importances, importances has already been created for you from model.featureImportances.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297955</td>\n",
       "      <td>LISTPRICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.222088</td>\n",
       "      <td>ORIGINALLISTPRICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129761</td>\n",
       "      <td>LIVINGAREA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.063817</td>\n",
       "      <td>LISTING_TO_MEDIAN_RATIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.058087</td>\n",
       "      <td>TAXWITHASSESSMENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.056933</td>\n",
       "      <td>TAXES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050260</td>\n",
       "      <td>SQFTABOVEGROUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.037233</td>\n",
       "      <td>BATHSTOTAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.030874</td>\n",
       "      <td>SQFT_TOTAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.013576</td>\n",
       "      <td>LISTING_PRICE_PER_SQFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    importance                  feature\n",
       "0     0.297955                LISTPRICE\n",
       "1     0.222088        ORIGINALLISTPRICE\n",
       "8     0.129761               LIVINGAREA\n",
       "38    0.063817  LISTING_TO_MEDIAN_RATIO\n",
       "7     0.058087       TAXWITHASSESSMENTS\n",
       "6     0.056933                    TAXES\n",
       "5     0.050260          SQFTABOVEGROUND\n",
       "15    0.037233               BATHSTOTAL\n",
       "40    0.030874               SQFT_TOTAL\n",
       "39    0.013576   LISTING_PRICE_PER_SQFT"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, Series\n",
    "\n",
    "# Convert feature importances to a pandas column\n",
    "fi_df = DataFrame(rf_model.featureImportances.toArray(), columns=['importance'])\n",
    "\n",
    "# Convert list of feature names to pandas column\n",
    "fi_df['feature'] = Series(col2vectorize)\n",
    "\n",
    "# Sort the data based on feature importance\n",
    "fi_df.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "\n",
    "# Inspect Results\n",
    "fi_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. We can see that now the features that are the most important are things like the area of the house and taxes both of which are highly correlated with the price of the home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading Models\n",
    "Often times you may find yourself going back to a previous model to see what assumptions or settings were used when diagnosing where your prediction errors were coming from. Perhaps there was something wrong with the data? Maybe you need to incorporate a new feature to capture an unusual event that occurred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# Save model\n",
    "rf_model.write().overwrite().save(file_path + 'rfr_no_listprice')\n",
    "\n",
    "# Load model\n",
    "loaded_model = RandomForestRegressionModel.load(file_path + 'rfr_no_listprice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, your model saves and loads successfully. Now you can come back and compare should you build another model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:playground_py36]",
   "language": "python",
   "name": "conda-env-playground_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
